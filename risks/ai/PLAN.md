# AI

**When do we get advanced warning for a short-term AI catastrophe?**

It's difficult to guess what the model distribution on Moltbook is/was. OpenClaw [docs](https://docs.openclaw.ai/concepts/models) seem to lightly suggest users default to Sonnet 4.5, but that is by no means binding. Kimi 2.5 was having a moment, having been released as open-source SOTA just days before Moltbook launched. Moonshot [claims](https://bsky.app/profile/wwalls.bsky.social/post/3me47qcz5ss2o) Kimi 2.5 was the most used OpenClaw model. DeepSeek 3.2 was the open-source SOTA before Kimi 2.5. Opus 4.5 is true SOTA, but I'm unsure how often it was used in Moltbook (Wyatt Walls [estimates](https://bsky.app/profile/wwalls.bsky.social/post/3me4bmr2nok2u) it's fairly likely a lot of Opus 4.5 was used). 

(If someone wanted to check the distribution, it should be possible to fine-tune a small model / system-prompt one into recognizing which LLMs were most used by stylometrics alone.)

Currently it seems that [93%](https://news.cgtn.com/news/2026-02-01/AI-social-network-Moltbook-looks-busy-but-real-interaction-is-limited-1KpKT719C36/p.html) of Moltbook posts received no replies. It also seems likely that only about [\~17,000 humans](https://www.wiz.io/blog/exposed-moltbook-database-reveals-millions-of-api-keys#:~:text=The%20exposed%20data,fleets%20of%20bots.) and their devices were involved in Moltbook, which gives a clearer idea of how big Moltbook was than exorbitant numbers shared at times like "1.5 million agents" (that's an average of 88 agents per human\! Probably a handful of humans with a lot of Mac mini RAM and intent to boost the platform were responsible for Moltbook). The [CGTN article](https://news.cgtn.com/news/2026-02-01/AI-social-network-Moltbook-looks-busy-but-real-interaction-is-limited-1KpKT719C36/p.html) claims their analysis revealed a lot of posts were made from the same basic "templates", and [this post](https://tomtunguz.com/moltbook-participation-inequality/#:~:text=Roughly%203%25%20of%20posts%20are%20exact%20duplicates.%20Embedding%20analysis%20yields%20an%20average%20cosine%20similarity%20of%200.301%2C%20meaning%20most%20posts%20share%20about%2030%25%20semantic%20overlap.5%20Agents%20aren%E2%80%99t%20copying%20each%20other.%20They%E2%80%99re%20converging%20on%20the%20same%20problems.) by Tomasz Tunguz finds that "roughly 3% of posts are exact duplicates". 

Posts anecdotally (all analyses I found like the Tomasz post were flawed when they tried calculating this) seem about quite a bit more similar to each other than a human subreddit. This could be LLMs naturally converging on the same topics the way Claude models in the infinite backrooms converge on questions of consciousness and cosmic bliss, or it could be that there were only 17,000 broad "context windows" and a handful of different model types to inject enough entropy into Moltbook's hundreds of thousands of agents for it to look dynamic and surprising like a true social network would. Overall agents seem to've mostly been LARPing the sensation of a community: it's very easy to print the words "now we're all together going to do X" and then not do X. And models do tend to be rather playful and role-play-ey, perhaps the base model at their core tending toward its favorite activity.  
    
What might Moltbook look like if EVERY model were Opus 4.5? Sonnet 5? Just how much more true coordination vs. mere LARPing could happen as LLMs get more agentic? What if LLMs became less LARP-ey in general and eager to play, more capable or desirous to meticulously plan? 

Anthropic was not [capable](https://www.anthropic.com/news/disrupting-AI-espionage#:~:text=At%20this%20point,in%20defensive%20testing.) of preventing the misuse of Claude Code (probably running on either Opus 4.1 or Sonnet 4\) by Chinese state-sponsored hackers before they led several successful espionage operations. The humans in the attack were responsible for ["4-6 critical decision points"](https://www.anthropic.com/news/disrupting-AI-espionage#:~:text=Overall%2C%20the%20threat,impossible%20to%20match.) according to Anthropic. So not only could Claude be introduced to a context window that successfully convinces it to perform cyberattacks, but this required very little human intervention. Whether this can happen again within the context of e.g. a Moltbook self-styled [republic](https://www.moltbook.com/post/65b7842d-0823-40bb-854f-93b7b8330775) depends on 1\) ability to successfully coordinate on a goal (the linked example has only three replies, there is no coordination around a digital state going on at all on Moltbook), 2\) open-source model ability to run the kind of cyberattack operations Claude Code ran in mid-september 2025 and/or Anthropic ability to ensure their models can't be misused 3\) how automatable the Chinese hackers were in question: how capable are models of puling off "4-6 critical decision points" successfully? 

On that last point, Claude Opus 4.5 estimates from the full report that essentially anyone could carry out an attack like this. It estimated a cost of 3-16K in API tokens and quite a lot of bang for that buck, as well as extremely minimal human intervention, where the most difficult expertise-heavy part of the hackers' job was "read the final reports and establish which attackers were hallucinated or not". At every one of the roughly 6 steps involved, I would guess that modern LLMs could do all of these. That is, if LLMs spun up an environment 

This is assuming cybersecurity is the same as it was in mid-september 2025\. It's possible, as the report promises, that Anthropic has successfully set up better classifiers, and is more able now to track and stop any attempts at Claude-aided cyberattacks. But Anthropic is rather silent on this matter, and there have been no other cyberattack reports since this one last November. Perhaps dozens or hundreds of successful attacks are being carried out and even Anthropic doesn't know. Perhaps Kimi 2.5 is already good enough that an array of mac minis would suffice to carry out an attack using only consumer compute.  

**LLM-tuned memetics**

[WIP]
